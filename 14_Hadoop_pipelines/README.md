# Методичка

## Установить hadoop

Скачать архив, разархивировать под тем пользователем с которого скачали Зайти под рутом и Переместить рахархивированную папку edx в корень /
Выполнить команду, чтобы добавить в .bashrc
переменные окружения

То есть выполнить следующее в терминале

```
cd $HOME

wget -nc https://drive.google.com/file/d/1YgSxm63cGnohwv-J3KdDBGZPzLTFOdaS/view?usp=sharing

tar zxvf edx.tar.gz

sudo mv edx /

sudo hostnamectl set-hostname server3

sudo bash -c "cat >> /etc/hosts" <<EOF
0.0.0.0		    server3
127.0.0.1	    server3
EOF

sudo bash -c "cat >> $HOME/.bashrc" <<EOF
. /edx/app/hadoop/hadoop/hadoop_env
export HADOOP_HOME="/edx/app/hadoop/hadoop"
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export SCALA_HOME=/edx/app/hadoop/scala
export SPARK_HOME=/edx/app/hadoop/spark
export PATH=$SPARK_HOME/bin:$JAVA_HOME/bin:$SCALA_HOME/bin:$PATH
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
EOF
```
Установить пакеты
```
sudo apt-get update

sudo apt install openssh-client openssh-server openjdk-8-jre openjdk-8-jre-headless python2 python-is-python2 openjdk-8-jdk-headless python-numpy

sudo ln -s /usr/lib/jvm/java-8-openjdk-amd64 /usr/lib/jvm/java-8-oracle 

```


установить Java-8-oracle как основной

    sudo update-alternatives --config java

Установка завершена. **Закройте терминал!!!**

## Команды YARN

```
cd /edx/app/hadoop/hadoop

sbin/start-dfs.sh

sbin/start-yarn.sh

jps

```

Если ранее ошиблись, то будет баг с ssh, устраняем так

    ssh-keygen -f "/home/vladimir/.ssh/known_hosts" -R "0.0.0.0"
    ssh-keygen -f "/home/vladimir/.ssh/known_hosts" -R "server3"


Примеры MapReduce
-----------------

Примеры расположены в кластере HDInsight в `/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar`. Исходный код этих примеров расположен в кластере HDInsight в `/usr/hdp/current/hadoop-client/src/hadoop-mapreduce-project/hadoop-mapreduce-examples`.

В архиве содержатся следующие примеры:

<table aria-label="Таблица 1" class="table table-sm margin-top-none">
<thead>
<tr>
<th>Образец</th>
<th>Описание</th>
</tr>
</thead>
<tbody>
<tr>
<td>aggregatewordcount</td>
<td>Подсчитывает количество слов во входных файлах.</td>
</tr>
<tr>
<td>aggregatewordhist</td>
<td>Создает гистограмму слов во входных файлах.</td>
</tr>
<tr>
<td><code>bbp</code></td>
<td>Использует формулу Бэйли-Боруэйна-Плаффа для вычисления знаков числа&nbsp;π.</td>
</tr>
<tr>
<td>dbcount</td>
<td>Подсчитывает журналы просмотра страниц, сохраненных в базе данных.</td>
</tr>
<tr>
<td>distbbp</td>
<td>Использует формулу ББП для вычисления знаков числа&nbsp;π.</td>
</tr>
<tr>
<td>grep</td>
<td>Подсчитывает совпадения регулярного выражения с входными данными.</td>
</tr>
<tr>
<td>join</td>
<td>Выполняет объединение сортированных наборов данных одного размера.</td>
</tr>
<tr>
<td>multifilewc</td>
<td>Подсчитывает слова в нескольких файлах.</td>
</tr>
<tr>
<td>pentomino</td>
<td>Программа для укладки фигур с целью поиска решений при игре в пентамино.</td>
</tr>
<tr>
<td>pi</td>
<td>Оценивает число&nbsp;π по методу квази-Монте-Карло.</td>
</tr>
<tr>
<td>randomtextwriter</td>
<td>Записывает 10 ГБ случайных текстовых данных на узел.</td>
</tr>
<tr>
<td><code>randomwriter</code></td>
<td>Записывает 10 ГБ случайных данных на узел.</td>
</tr>
<tr>
<td><code>secondarysort</code></td>
<td>Определяет вторичную сортировку для этапа редукции.</td>
</tr>
<tr>
<td>sort</td>
<td>Сортирует данные, записанные случайным образом.</td>
</tr>
<tr>
<td>sudoku</td>
<td>программа решения судоку.</td>
</tr>
<tr>
<td>teragen</td>
<td>создание данных для программы TeraSort.</td>
</tr>
<tr>
<td>terasort</td>
<td>выполнение программы TeraSort.</td>
</tr>
<tr>
<td>teravalidate</td>
<td>проверка результатов выполнения программы TeraSort.</td>
</tr>
<tr>
<td>wordcount</td>
<td>Подсчитывает количество слов во входных файлах.</td>
</tr>
<tr>
<td><code>wordmean</code></td>
<td>Подсчитывает среднюю длину слов во входных файлах.</td>
</tr>
<tr>
<td><code>wordmedian</code></td>
<td>Подсчитывает медианную длину слов во входных файлах.</td>
</tr>
<tr>
<td>wordstandarddeviation</td>
<td>Подсчитывает стандартное отклонение в длинах слов во входных файлах.</td>
</tr>
</tbody>
</table>

Запуск примера для подсчета слов
--------------------------------
шпаргалка
```
bin/hadoop fs -ls /

bin/hadoop fs -ls /data

bin/hadoop fs -ls /output/

bin/yarn jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar' wordcount  /data/tracking.log  /output/tracking.log

bin/hadoop fs -ls /output/tracking.log/

bin/hadoop fs -cat /output/tracking.log/part-r-00000
```


1.  Подключитесь к HDInsight с помощью протокола SSH. Замените `CLUSTER` именем кластера и введите следующую команду:
    
        ssh [email protected]
        
    
2.  В сеансе SSH используйте следующую команду, чтобы вывести список примеров:
    
        yarn jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar'
        
    
    Эта команда создает список примеров из предыдущего раздела данного документа.
    
3.  Чтобы получить справку по конкретному примеру, используйте следующую команду: В данном случае запускается пример **wordcount**.
    
         yarn jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar' wordcount
        
    
    Отобразится следующее сообщение.
    
        Usage: wordcount <in> [<in>...] <out>
        
    
    Это сообщение означает, что можно указать несколько входных путей для исходных документов. Окончательный путь — это место, где сохраняются выходные данные (число слов в исходных документах).
    
4.  Для подсчета всех слов в книге "Записи Леонардо да Винчи", которая поставляется с кластером и служит примером исходных данных, используйте следующую команду:
    ```
    hdfs dfs -put /edx/app/hadoop/hadoop/davinci.txt /data/davinci.txt

    yarn jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar' wordcount /data/davinci.txt /output/davinci
    ```
    
        
    
    Для этого задания считываются входные данные из файла `/example/data/gutenberg/davinci.txt`. Выходные данные для этого примера сохраняются в `/example/data/davinciwordcount`. Оба пути расположены в хранилище по умолчанию для кластера, а не в локальной файловой системе.
    
    Примечание
    
    Как сказано в справке по примеру wordcount, вы также можете указать несколько входных файлов. Например, команда `hadoop  jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar' wordcount /example/data/gutenberg/davinci.txt /example/data/gutenberg/ulysses.txt /example/data/twowordcount` позволит подсчитать слова в файлах davinci.txt и ulysses.txt.
    
5.  По завершении задания воспользуйтесь следующей командой, чтобы просмотреть результат:
    
        hdfs dfs -cat /example/data/davinciwordcount/*
        
    
    Эта команда сцепляет все выходные файлы, созданные заданием. Она отображает выходные данные в консоли. Результат будет аналогичен приведенному ниже:
    
        zum     1
        zur     1
        zwanzig 1
        zweite  1
        
    
    Каждая строка соответствует одному слову и частоте его появления в исходных данных.
    

Пример судоку
-------------

[Судоку](https://en.wikipedia.org/wiki/Sudoku) — это логическая головоломка, которая состоит из девяти полей размером 3 x 3 клетки. Некоторые ячейки в клетках содержат числа, остальные ячейки пустые. Задача заключается в поиске чисел для пустых ячеек. С помощью указанной выше ссылки можно получить дополнительные сведения о головоломке, однако целью этого примера является поиск значений для пустых ячеек. Таким образом, на входе программы должен быть файл в следующем формате.

*   Девять строк и девять столбцов
*   Каждый столбец может содержать число или знак `?` (означает пустую ячейку).
*   Ячейки разделяются пробелами




Следующая задача — составление головоломки судоку, по правилам которой не допускается использование одного и того же числа в строке или столбце. Ниже приведен пример правильно созданного кластера HDInsight. Он находится в `https://github.com/naver/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta` и содержит приведенный ниже текст.

    8 5 ? 3 9 ? ? ? ?
    ? ? 2 ? ? ? ? ? ?
    ? ? 6 ? 1 ? ? ? 2
    ? ? 4 ? ? 3 ? 5 9
    ? ? 8 9 ? 1 4 ? ?
    3 2 ? 4 ? ? 8 ? ?
    9 ? ? ? 8 ? 5 ? ?
    ? ? ? ? ? ? 2 ? ?
    ? ? ? ? 4 5 ? 7 8
    
```
vladimir@server3:/edx/app/hadoop/hadoop$ hdfs dfs -put /edx/app/hadoop/hadoop/puzzle1.dta /data/puzzle1.dta

vladimir@server3:/edx/app/hadoop/hadoop$ bin/hadoop fs -ls /data
```

Чтобы обработать эти данные в примере судоку, используйте следующую команду.

    bin/yarn jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar' sudoku /data/puzzle1.dta

    

Полученный текст должен выглядеть следующим образом.

    8 5 1 3 9 2 6 4 7
    4 3 2 6 7 8 1 9 5
    7 9 6 5 1 4 3 8 2
    6 1 4 8 2 3 7 5 9
    5 7 8 9 6 1 4 2 3
    3 2 9 4 5 7 8 1 6
    9 4 7 2 8 6 5 3 1
    1 8 5 7 3 9 2 6 4
    2 6 3 1 4 5 9 7 8
    

Пример "Пи" (π)
---------------

В примере «Пи» используется статистический метод (квази-Монте-Карло) оценки значения числа пи. Точки в произвольном порядке помещаются внутри единичного квадрата. В квадрат также вписан круг. Вероятность того, что точки находятся в круге, равна площади круга, π/4. Значение pi можно оценить на основе значения `4R`. R — это отношение количества точек, находящихся внутри круга, к общему количеству точек, находящихся внутри квадрата. Чем больше выборка используемых точек, тем точнее оценка.

Для запуска примера используйте следующую команду. Для оценки числа π в команде используется 16 карт с 10 000 000 примерами в каждой.

    bin/yarn jar '/edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar' pi 16 10000000

    

Команда должна возвращать значение наподобие **3,14159155000000000000**. Для справки, первые 10 знаков числа пи: 3,1415926535.

Пример GraySort размером 10 ГБ
------------------------------

GraySort — это измерение производительности сортировки. Его показателем служит скорость (ТБ/мин), достигаемая при сортировке очень больших объемов данных, обычно не менее 100 ТБ.

В этом примере используется небольшой объем данных, 10 ГБ, чтобы можно было выполнить сортировку достаточно быстро. В ней используются приложения MapReduce, разработанные `Owen O'Malley` и `Arun Murthy`. Эти приложения победили в 2009 году на конкурсе приложений сортировки общего назначения (Daytona) для больших объемов данных, показав скорость 0,578 ТБ/мин (100 ТБ за 173 минуты). Дополнительные сведения об этом и других измерениях производительности сортировки см. на веб-сайте [Sort Benchmark](https://sortbenchmark.org/).

В этом примере используются три набора программ MapReduce.

*   **TeraGen**: программа MapReduce, которая создает строки с данными для последующей сортировки.
    
*   **TeraSort**: производит выборку входных данных и использует MapReduce для сортировки данных в общем порядке.
    
    TeraSort представляет собой стандартную сортировку MapReduce, за исключением настраиваемого разделителя. В разделителе используется отсортированный список выборки ключей N-1, определяющий диапазон ключей для каждой функции reduce. В частности, все ключи, подобные этому примеру \[i-1\] <= key < sample\[i\] отправляются для сокращения i. Этот разделитель гарантирует, что выходные данные reduce `i` будут меньше, чем выходные данные reduce `i+1`.
    
*   **TeraValidate**: программа MapReduce, которая проверяет глобальную сортировку выходных данных.
    
    В выходном каталоге создается одна функция map для каждого файла, и каждая функция map гарантирует, что каждый ключ будет меньше или равен предыдущему. Функция map создает записи первого и последнего ключей каждого файла. Функция reduce гарантирует, что первый ключ файла i больше последнего ключа файла i-1. Все проблемы указываются в выходных данных этапа редукции вместе с неотсортированными ключами.
    

Для создания данных, сортировки и проверки выходных данных используйте следующую команду:

1.  Создайте 10 ГБ данных, которые сохранятся в хранилище по умолчанию кластера HDInsight в `/example/data/10GB-sort-input`.
    
        yarn jar edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen -Dmapred.map.tasks=50 100000000 /example/data/10GB-sort-input
        
    
    Ключ `-Dmapred.map.tasks` говорит Hadoop о том, сколько задач сопоставления будет использоваться в этом задании. Последние два параметра означают, что задание создаст 10 ГБ данных и сохранит их в `/example/data/10GB-sort-input`.
    
2.  Выполните следующую команду, чтобы отсортировать данные:
    
        yarn jar edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar terasort -Dmapred.map.tasks=50 -Dmapred.reduce.tasks=25 /example/data/10GB-sort-input /example/data/10GB-sort-output
        
    
    Ключ `-Dmapred.reduce.tasks` говорит Hadoop о том, сколько задач сокращения будет использоваться в этом задании. Последние два параметра соответствуют путям расположения входных и выходных данных.
    
3.  Используйте следующую команду, чтобы просмотреть отсортированные данные:
    
        yarn jar edx/app/hadoop/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teravalidate -Dmapred.map.tasks=50 -Dmapred.reduce.tasks=25 /example/data/10GB-sort-output /example/data/10GB-sort-validate
        
    
____


# Spark



Запустим spark

```
cd /edx/app/hadoop/spark

sbin/start-all.sh

jps
```


Запустим те же примеры, что были выше но используя Python

```
hdfs dfs -mkdir /log

bin/spark-submit --master local /edx/app/hadoop/spark/examples/src/main/python/pi.py

bin/spark-submit --master local /edx/app/hadoop/spark/examples/src/main/python/wordcount.py /data/tracking.log
```

# Что такое machine learning pipeline?

конвеер машинного обучения - это серия взаимосвязанных этапов обработки данных и моделирования, предназначенных для автоматизации, стандартизации и упрощения процесса создания, обучения, оценки и развертывания моделей машинного обучения.

конвеер машинного обучения является важным компонентом в разработке и производстве систем [машинного обучения] (https://www.ibm.com/topics/machine-learning), помогая [ученым-ученым] (https://www.ibm.com/Themics/Data Science) и инженеры по данным управляют сложностью процесса сквозного машинного обучения и помогают им разработать точные и масштабируемые решения для широкого спектра приложений.



## Machine learning pipelines  много преимуществ.

* **Модуляризация**: конвееры позволяют вам разбить процесс машинного обучения на модульные, четко определенные шаги.Каждый шаг может быть разработан, протестирован и оптимизирован независимо, что облегчает управление и поддержание [Рабочий процесс](https://www.ibm.com/topics/workflow). 
     
    
* **Воспроизводимость**: конвееры машинного обучения облегчают воспроизведение экспериментов.Определяя последовательность этапов и их параметры в конвеере, вы можете точно воссоздать весь процесс, обеспечивая последовательные результаты.Если шаг не работает или производительность модели ухудшается, конвеер может быть настроен для повышения оповещений или принятия корректирующих действий.
    
    
* **Эффективность**: Pipelines [Automate](https://www.ibm.com/topics/automation)-Лабелинг), Инженерная инженерия и [модель](https://www.ibm.com/topics/data-modeling) Оценка.Эта эффективность может сэкономить значительное количество времени и снизить риск ошибок.
    
    
* **Масштабируемость**: конвееры могут быть легко масштабированы для обработки больших наборов данных или сложных рабочих процессов.По мере роста растущей сложности данных и моделей вы можете настроить конвеер, не перенастроил все с нуля, что может быть трудоемким.
     
    
* **Эксперименты**: Вы можете экспериментировать с различными методами предварительной обработки данных, выбором объектов и моделями путем изменения отдельных шагов в конвеере.Эта гибкость обеспечивает быструю итерацию и оптимизацию.
    
    
* **Развертывание**: конвееры облегчают [развертывание] (https://www.ibm.com/topics/continoury-deployment) моделей машинного обучения в производство.После того, как вы установили четко определенный конвейер для обучения и оценки моделей, вы можете легко [интегрировать] (https://www.ibm.com/topics/data-integration) в вашем приложении или системе.
    
    
* **Сотрудничество**: конвееры облегчают сотрудничество команд и инженеров данных.Поскольку рабочий процесс структурирован и документирован, членам команды легче понять и внести свой вклад в проект.
     
    
* **Управление и документацию версий**: Вы можете использовать системы управления версиями для отслеживания изменений в коде и конфигурации вашего конвеера, гарантируя, что вы сможете вернуться в предыдущие версии, если это необходимо.Хорошо структурированный конвеер поощряет лучшую документацию каждого шага.
    
## Стадии машинного обучения конвеера

Технология машинного обучения продвигается быстро, но мы можем определить некоторые широкие шаги, связанные с процессом создания и развертывания машинного обучения и моделей глубокого обучения.

1. **Сбор данных**: На этом начальном этапе новые данные собираются из различных источников данных, таких как базы данных, [API](https://www.ibm.com/topics/api) или файлы.Это употребление данных часто включает в себя необработанные данные, которые могут потребовать предварительной обработки, чтобы быть полезной.
    
    
2. **Предварительная обработка данных**: Этот этап включает в себя очистку, преобразование и подготовку входных данных для моделирования.Общие этапы предварительной обработки включают обработку пропущенных значений, категориальные переменные кодирования, масштабирование численных функций и разделение данных на наборы обучения и тестирования.
     
    
3. **Функциональная инженерия**: [Feature Engineering](https://www.ibm.com/topics/feature-engineering)-это процесс создания новых функций или выбора соответствующих функций из данных, которые могут улучшить прогнозирование моделивласть.Этот шаг часто требует знания и творчества.
    
    
4. **Выбор модели**: На этом этапе вы выбираете соответствующий алгоритм (ы) машинного обучения на основе типа проблемы (например, классификация, регрессия), характеристики данных и требования к производительности.Вы также можете рассмотреть настройку гиперпараметра.
    
    
5. **Обучение модели**: Выбранная [модель (s)](https://www.ibm.com/topics/ai-model) обучаются на наборе учебных данных с использованием выбранного алгоритма (ы).Это включает в себя изучение основных моделей и отношений в рамках учебных данных.Предварительно обученные модели также могут быть использованы, а не обучение новой модели.
    
    
6. **Оценка модели**: После обучения производительность модели оценивается с использованием отдельного набора данных тестирования или посредством перекрестной проверки.Общие показатели оценки зависят от конкретной проблемы, но могут включать точность, точность, отзыв, F1-показатель, среднюю квадратную ошибку или другие. 
     
    
7. **Развертывание модели**: После того, как будет разработана и оценивается удовлетворительная модель, его можно развернуть в производственной среде, где она может сделать прогнозы на новые, невидимые данные.Развертывание может включать создание API и интеграцию с другими системами.
    
    
8. **Мониторинг и обслуживание**: После развертывания важно непрерывно отслеживать производительность модели и перепродать ее по мере необходимости для адаптации к изменению шаблонов данных.Этот шаг гарантирует, что модель остается точной и надежной в реальном мире.
    

Машинное обучение [жизненные циклы](https://www.ibm.com/topics/data-lifecycle-Management) может варьироваться в зависимости от сложности и может включать дополнительные шаги в зависимости от случая использования, таких как оптимизация гиперпараметрии, перекрестная проверка и функциявыбор.Целью машинного обучения является автоматизация и стандартизация этих процессов, что облегчает разработку и поддержание моделей ML для различных приложений.
